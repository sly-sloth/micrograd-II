# micrograd-II
My iteration of Andrej's Karpathy's micrograd project.

## What's different?
- My own implementation from scratch.
- Classes for each element like Value, Neuron, NeuronLayer, MLP (nn), Optimizers etc.

## How to use?
- Simply clone the repo and import the classes and functions as per requirement.
- The whole tool is structured in form of a package for ease of use.
- The flow is similar to that of pytorch library.

## What's upcoming?
- CrossEntropyLoss along with softmax derivative. Would require Jacobian matrix, thus can't have scalar implementation.
- Redesign
